{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pFHxIL9yv07"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bT8r4RGyv1A"
   },
   "source": [
    "# Demo: CNN with Keras\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dfgYO-oyv1D"
   },
   "source": [
    "## Faces classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH3F0aLlyv1H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmF9-C-syv1Q"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DhL5fvqyv1T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFuJDWijyv1Y"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpYq76vyyv1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to C:\\Users\\Jewin\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# The faces dataset\n",
    "faces = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnadpkG2yv1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _olivetti_faces_dataset:\n",
      "\n",
      "The Olivetti faces dataset\n",
      "--------------------------\n",
      "\n",
      "`This dataset contains a set of face images`_ taken between April 1992 and \n",
      "April 1994 at AT&T Laboratories Cambridge. The\n",
      ":func:`sklearn.datasets.fetch_olivetti_faces` function is the data\n",
      "fetching / caching function that downloads the data\n",
      "archive from AT&T.\n",
      "\n",
      ".. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "\n",
      "As described on the original website:\n",
      "\n",
      "    There are ten different images of each of 40 distinct subjects. For some\n",
      "    subjects, the images were taken at different times, varying the lighting,\n",
      "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
      "    details (glasses / no glasses). All the images were taken against a dark\n",
      "    homogeneous background with the subjects in an upright, frontal position \n",
      "    (with tolerance for some side movement).\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   =====================\n",
      "    Classes                                40\n",
      "    Samples total                         400\n",
      "    Dimensionality                       4096\n",
      "    Features            real, between 0 and 1\n",
      "    =================   =====================\n",
      "\n",
      "The image is quantized to 256 grey levels and stored as unsigned 8-bit \n",
      "integers; the loader will convert these to floating point values on the \n",
      "interval [0, 1], which are easier to work with for many algorithms.\n",
      "\n",
      "The \"target\" for this database is an integer from 0 to 39 indicating the\n",
      "identity of the person pictured; however, with only 10 examples per class, this\n",
      "relatively small dataset is more interesting from an unsupervised or\n",
      "semi-supervised perspective.\n",
      "\n",
      "The original dataset consisted of 92 x 112, while the version available here\n",
      "consists of 64x64 images.\n",
      "\n",
      "When using these images, please give credit to AT&T Laboratories Cambridge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(faces.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wO-oI0WUyv1k"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALQAAAC0CAYAAAA9zQYyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAayUlEQVR4nO2dS5MkxZVGbzUSIBAgHo0aTAaGZKYfzd9iwwpsFoCg1YIGSTyb1mLsyzoVedw9M5mxGSLvt6ksjwgPd4+I+/L7uHn69Gk1GnvBvf/rATQa/5PoF7qxK/QL3dgV+oVu7Ar9Qjd2hd+cc/Irr7zy9MGDB1VV9cwzz1RVFa0kNzc3VVV1797td8LfQa75+eefD238neOnWmBsDATb8tvO41hzfNU3j9t4Z/dbHbe+bZ1GyPFV36v+ZufaGE8dF3//9NNP036+//77qqr69ttvD///9NNPRxM764V+8OBBvf/++1VV9dJLLx0N5De/+e/unn/++UPbCy+8UFV3X5Yff/zxzuC2v7/77ruqqnry5Mmhjb+DPCg+5IyBx/Px8fdzzz13dB7bnn322aP7po3tP/zww6Eta8EXKPfjtRxPxsu2gH1nzbI2bBshY7QXms8tz4b348uU9eVa5Hq2ZTyjfoK8nLz+yy+/PLTles71o48+qqqqDz74oKqqPvzww6N+q858oW9ubo4Wni9vHs7vfve7oza+0GnjQ+Y1mTAnbouVxbAXlv3/9re/Pbo3H3KOczz2kvNjyb15TcbGh5zxjMZoH13ANclxziVrwheb97a5Bhy3UXJ7eW2MxoGJfAz8gLimeca8d8bGj+HVV1+tqqo33nijqu4+izvjmY6m0fiV4WIKbTKmfcG8dnveirLySw4V4pduYggpmLH7fNkcd6iMjYfjJjUK5aG4k+PGzk2O533Yt61j5kVqnHmRa/373/8+upb9zfQXrpOJM5yXjXumL/EYn1soNJ91RA1S4Yi4r7322tGciKbQjV2hX+jGrnCWyFF1TOrJPsJWyM4CM9WYmazKNf8oSDzP2CdFjpxrCsTK3Gjjsus5f1PwcnwkUpi4Y+flfmTNsQxxftY3kbUy8YltXEfrL2KOraMpriPl0Z61PY/0GeWwlcLGVeBspXD7pZGC5WskFTFqbV/gypQ1+iK3WFG6wBTKc2BK4cxMNuJGRrnSRuqXvqkApo32Wq5TxmbPw0yLI7t2qCOvMXNc+rE2u3Z0PPfj3kQU1pdffvnOdVs0hW7sCv1CN3aFi5VCU2aMDQW0cebale3VbLera2a20O3vYKY0jkQTO27zMjs0Ye1ZR1tbG89KxGM/eTYrpZDPMKIIxzpT/Fcil7lBUJE0W/p2d3m0nk2hG7vC2RQ6mHnbsc0oQq4dKXqjr69qTYFMSVvtLpqvQs4zblPlO5dmtjR/kxEl3MIchEbHgxXXyprM1mF0/WoMpyraZmal3469Hxlbzm8K3bgK9Avd2BUudk4ydmcK0IxdWVuVOwbZ/cLizEGIx1f3NCfziBSjIAS7tynFJlaZM/vMHs2+TcEbOdSbPd92+Gxc5qZ7qo+0rbHZpqtuRQ1eYwaAU4MHmkI3doWLlcKZ4mZumKZwma9Cle/CzfwkzFmfMApOChSQisxMkGzncVOKVxQlFMoouSmS//rXv46Oc6fQKD2pqJnW8nsUDWPRKdtjVbdramFio+CB2biJmb/NnX6mRxuNXxn6hW7sCheLHMYiT931CsshWxspXzPkesY10qEl4zFHHLJKU2bC2ka21bBXE0ksUHcUrZ1+uBb5bSIZx2NikwWgUpSwaBDrj9fMHJFmUUqEuZRWuSPWTLlsO3TjqtAvdGNXOFvk2NpsV7bJU6NBVtvTdl6cWMzOynbT8slyc40F045Y6kzzt3QIyU/CNl7PeUVsMDZMkSrH2UZ2nn6++eabQ9s///nPo/NMvCK7t3QJmSNTLUQcoLOUiSbsO+dyDhSbzkVT6MaucBaFfvr06VHGIH5NUST4tZm9Nhjt8AWkZKYAffXVV0f9UJkJBTcHKovsSKh8VdXvf//7qnK7NsdrTleWgcmiT6pu14XrGCrKeWX+X3/99dG1OZ9t7JO261xPF82Me6S4zqKAqJDP2kZKXDgXx2MKcJ6HKZFEU+jGrtAvdGNXOFvk2GaBJLszdsZskUFYG5UMi7rgcduezf1GCmXONdHGtpUpcrz44otH4+I1aacCaEpRWCXFDMuHx3WMEmcOQjwvx6n0cS0yf659ns3KDm9KnIlffB4RNdhm13D+lsfQ7M95j0yEI5pCN3aFsyj0zz//fPiS8hVSSYmSRsoahcRS45oJju1UZtJGapQxjMx/RnnyhZNiRjEx5WfkCmrKLilPELPWKDXwdj2rbtfCKCvX1igwxxOqxmssVa2ZIE0JJ1XMmnFNQqHDlapuuRXbeE24o2VDPScX9qG/k85qNH4l6Be6sStc7Jw0A1mcsQrbeaLoElbJrO5hq2SpEXFGwbY5l8fDAqP0Vc2Vx1GWo7TbziTndWrqYHOwMrFgJXJYat3VjmP6HO3QpR+KJFGgOZf0w7XN/P70pz8d2kwkMZ/2WdWGEZpCN3aFsyn09gsZZb4MzL/DKPCjR48Ov6MMkkKlWJHtqBE0vUWBpBktudH+8Ic/HF1LWCSJmRYt7s/WJPfd9pN1IVUz6h/qR+U5Yxy5fQZck4zDzJs2l6pb7sF7v/XWW0dz/fjjj6uq6vHjx0fz43N79913j8Zj/i+WkbYjVhpXhX6hG7vCWSLHvXv3juyqZHFhpVTwTOGIqEDFhDbn9Jnk1lVV77333tH9woYoelDhMCXl7bffrqq7bC+K0sOHD4/mbLZnzoduoWbjzng4F44n4LyixHHNrFqUKZyWQzCVo6qqXnnllaNxf/LJJ0f343jyrCk2vfPOO0dtmWtKsFXdipLsjyKJuelGKbSE5ytnp6bQjV2hX+jGrnC2yBGN14I/w+IpPpgtNGyD0Q78HZZMVhkWmbJexGeffXb4Tba5LQVWdStqxGrCa8gWYzmgfZiY+UOTDefeFDnI7m3cuXfs7FW3ogRFiogPI3/wrDnFsIyDLDv3/sc//nF0bdWtqPX6668f2vKbfds7kfUzR6sq92+OeEHHpm1F4pG1oyl0Y1c4O7ddFL/t3yp3FQzVoqIYykmqRKpltUECUsxQWSphvE/aSVnCCcwZyDI1mXsowWsyB9q4w2U4PyqFFsmTc3le7m1KKKkV1zRKN8eYNq6tcT1zmyVXy3FL1UtulPNGu5lW8jnPmGPIc8tcWilsXAX6hW7sChc7J1nERn6TLYZNcdvUnGo+//zzw+8ol2RNpgSEJXMM9+/fP/w2x6j0yb7D9jnusEAqq0T6JquMkkYWnuutIGiVZ9TP2prSTDHEglLpvJR7WpAs1yz3M3s1x0sRwFIobMfKa3g/XpM5cO3zm/PaOiy1yNG4ClxMofPFmTBPU47VLwnFIIV+8803p8dD1S0f3ihBirmcWjSEuYqawmVzJWWx3bNQ4HOSkueeZrbimsyurfIcgrMol5GLq43HdilDjbkm6XsUm2mplU0pnOW7I5pCN3aFfqEbu8LFue0sZ11YjaWvJaLY0BZMNh2Rg8pDlEpLkUCWyntbnZRTC2bmPI7fdvuoPEUZpAJkAcGWyckURWPDFOdOhTk0cQxZx5Hokt8UJdK2qsG+Sru8LeRK2PNY1ZVpCt3YFc5ONLONBzRTjoWpW8JrcxmsuqVGNDeZEhYKP0oRYKH427FWefqBwHarOB4zrdmOI8Exzigcj+W37VZavRSO3XLNGWddlW82Cm6+LLZ7OJqXuYOGClv0zaq4Z1Poxq7QL3RjVzhbKdzafo0FkKUYi0yb1SKpchuw7UKuimOayGFscSZyjEQCy5E3KxhpGZ14va2jsXgrRso2S0Bu2Z0sw9JIVDBRxJS4UxW8Vd92zdYA0Eph4ypwdm67rfshKbCVKQhW1ZKM8pjZysxyoxLKOW7UwSirlfklaB405JpVSWcrSXGq66qV1yAsjx/TGITDWYqE0fyMi8yqUc0SpG+vMbOmVd3a7vA2hW5cBfqFbuwKF6fTtQpUAR1fTKla5Y0LeHwmchB2P7Inu0/YmLmrss1Y5an1W0b1ZGznLtdT5IgYx7akb1jZtWlLtxhJmz+f4UyEsPW0+FFzSKryNMGzymArNIVu7Ar9Qjd2hYtrrIT1WS0OsiELdLXMOKfW915ZSyzlrcEKhq5StdoWu4kS5thj9+M9TfyybXcTKcwiU3W7VrzGtufNimHiHO9jdviZRceKsRLm025tVpSUaArd2BUuLrxpLodGZY2SWZTCqp6GUZuT626I4mKVs0ZU1PoJBRtFy2yvGe1m5p42F94vlJVKnd3PqD/XOeu3Usit5oklizGOabueKxs/22xtV26jQVPoxq7QL3RjVzjbOWkbeEn7YI6RhYVN0ako7HMUIWGwrV9zBlo5+ZhTlbVZ8U/bLiarNGXHFOCR//Zs3DO3gpWNluOOormqvGA2d5vfqoa7Fdbkmlph1tk1Jh4STaEbu8Ivjim0HR46LFnagHzVVDwsqoIwKmHmJlM0TSGzdAhGWRg1YdW9TnUF5bgtY+uKY+R6rm1+sxgpx8hsooG5ypoJzuZg5kjjDraOVhumytMqmLNUuEy7jzauCv1CN3aFizMnWR2QsECyaXNOsTJiJrqMfGi3YxgFiW7PY9+8n9W6zryYSsEckczua/ZhKmGrutamaOU8Xpt0EEwhzLVPwnTuOCYNgjlQ2f04L2K2s8trbX4mSswUQP5ukaNxVbh4pzBfCpUUizQIVhEgo+yc2+stHo2Ufkbdqpwi5BpW0zIqYfGR5t9gWFE827k0zmOxl0zSY6WjmZwmczTlcjTeWfyorb1RYCvzzHNpWrQMsfndhTcbV4V+oRu7wsVKYWDREmabXTnSjFwgZ/2E/Yx23kzZy29mZQo7o600bVTCVtEwVhwzfVrVKbabXdzs+VynsHFTwqtuFVoqjcnPx2vMgWqVn8/S21ox1syV87N1HqV5CLYGgE543rgK9Avd2BV+sR3anHPMr5jnhXXbVnLVuK7JFmYNWDkYhd1R5DBHK7PimGXENHHCaohwfrM8IZxXxvvFF18c2j7++OOjcZnbAG3p+W1+1RSpuGazwGITFSy4dbROOb7Kt7J9Z9oO3bgKnE2hLQVBYO6jZs+1uiNWGncFs49aEUpzTaTNOdeTcmQ8X375pd7bErCHipptepSWN3MlxTROl3GwWlio9SiG06qEZbzkGEbtSK0t6mh7rOp2/cgR7BmYw5dxN+MSXWOlcVXoF7qxK1ycTtdqbMyiL0bOKQYrxxZWurKFWjpd+guHvVq6A44/1zx+/PjQRlaa9mwvV1U9evToaAyp+83tZ1PILDqFc4048/Dhw6P7sc4LRYnMi8fNqcwciEyMM+ct9mNZkMxWbu+Cbf1zTfIsLeM/0RS6sSuc7Zy0jTZZpXQ9tY2Ki7lhzhSSUVyf1eiwvHHphwqVRVrQZBbqyKiQUGtSqJyXEtFVnl9uVTksc+EYzLzFeZkiZVXALHH8KhIn46FybdEnprjTZBqsku9s+2mzXeMq0C90Y1e4OEjW8thZHrdZFp2RTTGsxtiejWVUys1qWFv9logAZNdRrqgI0iYd8cSy45tyNcoclLmSTZvt2opfmoJEcSbnsp9ZmodR3sBtgGrVrXhhtVr4/COynRPIPBtj7tsiR+MqcHHEipl6gkuyihqlt2KMp+ZSq7qluNx5DNVim5kgQ9X+/Oc/H9qoABqFCiVj3+arYUqa5Z8zykoukmtIlckxYjLkeEypWiUtN05oxTFtV9gUV3NdtSTpqygeQ1Poxq7QL3RjVzhL5Li5uTmKsLB6IaN6IsHKycV2zwJzYjE2XHXLasm6LerEdgrTTyI8qqr+8pe/HN2TO4VmXw1Gue2iNFl9dBM5KFLkGnN2Yj+2e2oRMiOl0JT4WVYqy/g0UuJmpflsjCs0hW7sChdXwQpWJSIsUblVflrFF4bacOfOcrKRWs/KZvDrDyW03G4c4x//+MfD71BKjie/qfSkjZyFVN3SM4TKWmoHUjLLbGoZXc2MyjFmnax8CK+x9ASmkK92j81oYM+c88+ahRu1L0fjKtAvdGNXuNgOPYtOMaejVS1vc028BKtdOrtfsKroRMT2Sxtw7L4UL6IoUszg/CyM30SgVWHSYFXzJfe2/hLhUnWX3VsmI3PxNTu01ZDhvWf7GRYr2RErjatCv9CNXeEXOycZyCpNe12lvJ1lUTJ7rQW3sp+V+BCNn1r+qn7LrAwbWbeVRKPdOA49Zqkwy4elfhi5GmT+NgeL2GEbrSBmGcm4LVs/n79FJ3EtzMnNUhsEXdatcVW4OGJlVvFpZZs2ZxejrPz6bYcv1/OYRamYQkoukHGbWyfnYkqspfclrKQxlZ033nijqm4jW6rcOWuWVGdEtYxDzSp+zWJC2V/VvPCorSMpNecwS1/Aa1axhEFT6Mau0C90Y1c4W+TYKgPG7ixbvYkXoygGc4YJazLxYpXVnTD2arVhLKORlZ4zkcQUPF7717/+9fA76/PWW28d2pJCgWlw02ZRPGxb2eHDxi39xDn2/5nN2ba5R/V0THE1RXLr5tBb342rwFkU+smTJ4edrxmFXkWxWGJ02ymzHTXCzDum2FjkA5HdTJrbosyxbVVvxZyuLG0Ck87kt5k6GcM4y/Y5cuwypyIzweXeo5yCs3QIpnBz/pn3yBXUzJqWaGZ2jztj1dZG41eKfqEbu8LZSmHYfFipiRKWF80Ut5VSSDYVpYisZlYnmu2roM2IOBQFInKsaqxY3zYv5pfjTmGUQebf+9vf/lZVVZ9++umhzUqmmT84EdZuidotvfEosmcWiWRiyqpmukW5mEJq0TcrNIVu7ApnR6xs4+bsq7WQ/FWJA8JcHPO10pRl2UUtubmlGjAqusoLZ8rnyu01x6lcmssp1+fvf//7UX+WIsH8IFaU1UpthDIziQ3XIsfZt7kAG6ewKlim4BNm/jwVTaEbu0K/0I1d4WLnJLL5LSwnmxWoNCWr6pblWL43YuWaasoO62JvYZmBRrZwS+VrO5cJkqWoRNYcxW9lSz+18KgF0VK8oOK7xcg5Kfe02uPELNcg126VQsH67jQGjatEv9CNXeHiwptxUlrZIQOyfbM9kuXkXItEMZZKqwFFCss3kd+Wg4LjtnohZk2xcmXsJ23mkENYdIrZym3L13y7q3xLP7+53jZX80XmXGOdMR9pm+soB4vNZ+ZPP3O5qGoK3dgZzs5tFwqQ2hqkWla21jIVZafMyiWzfWbXrbqlQMz3Rvtp7rMqRRxYeWYmPLcaLLTLG/WwHHBGXcxlklmZZvnluHacazgX7cuWIiFrxvtx3rN0uvaMLN0BYVmrVpE/p6IpdGNX6Be6sSucJXLcu3fvwL4icpDlznyNKQqYHy+VGUvVa5ENlpWJ/VhEh/WzPVblTjVkw/m9EovMFmzRGSsHIhNTjF3bb15rkT+WxoAiiaUnsH0Is83b+lgKhVVlhs6c1LhKXKwURuFg4UUz/2RnynbczIVxe78gX6aZ2yxxSdUt9zAlw/LYmQmOfZNC57jt8FkqhVE0yKwgqSnNRlkttUGV5xoMaOq0WMBVyoJZ+WJzRLL6NVXOZbfzIyzFw53j2tpo/ErRL3RjVzh7p3BbAo2sNCzZnE+s0OWdgYgfL0WAHKcIkD6pcFIEMp9dy3of9kU7bOZA8YHjmRWMtFrXhKUdWGWbsl1Pc/wislZcH9sXsEBVYlaazfLhWeYkPvOVT/cMvVPYuCqcrRRuKTMphsX42VfPnb3AolxMKSKVyFdtlJznGkWwnUmjtiOqZTF1GQd35nJ8VBrZXEAtj53t1pmibAUzR9lZA5ujJbShiTZjs8KbHLfFZq4KfW7vUbXO43foe3q00fiVoV/oxq5w9k5h2JvVGAnLpb3WbKA5bmytyneuwrrpfLM9tu3HxKL0TZHDFA0LHF0VEZ0pOKN6KeZKa4UnzRa8ndNobJekN14V2cw1FDnyO66lvLetd9VasQ0sUbuhKXRjVzjbbLf94kgx84Xya7TSBenDdtS212/bSLXCHczkx7FZ6WSj5JZUZuQnEkWKxzM222WzjKw8l9cYhTb/jmDk8zHLP0dFMHO1dAcE4yJNIc+zJtfOTvHIVdiy0/4SNIVu7Ar9Qjd2hbPt0FuWTYXLYvzC2la1vI3l2DW2c8f+GD9nYoOJSvltTjNUQmyuZM3m0JTjVJRXuf9OTWNgYpjZ0i1W0tw6mb6XiKhBO7TtJWQcn3322aEtz8ZiPatu35WV4ppn2O6jjatCv9CNXeFsK8c2wmCVIiC/V2H8hNVBmWXlsWgPjpHWibBpC34lS7XIllHixlmb+TETZoe2yA8bg5XYW6UQiFhkkTiPHz8+tNFRyxJFZs14788//7yqqr744oujMdBCQpH0wYMHVXVXBJxFsbQdunFVuLg0cr4yfsn5grm7ZulUzca5gjnsBFTCvvrqq2k/r7322tE1RhEs2sPiGW0cq8pg5rxlDkRGoc2ea/n1+JucI8+L5yV9r1HlKk9YE/uyJZhnP5988klV3eXafD8ytlDq7fER2n20cRXoF7qxK/xipZDCvCkhiSDheauyb9v+RsfNqYjsNcoJFcCwQ9pRY38lSzWld5X9aKa42vYz+7SUuCvlOeB5VGwt117mT5uznUfFLfVhLAOTiRxE5s/7vfrqq4ff6ZNrcv/+/aq6+zwi9qzs1U2hG7vC2RTakl8H9tVmh8gcX0ZRHOY+eupYSFFDrWiOSpt9/Rb3Ztk8OXYzLa52Qo1aW9Idu8aS1PA8xlQaxQyFpuJmLrlsCzezeEYzI9pcRkpx5mDPiOWi45Jq8ahEU+jGrtAvdGNXOLvGyjYw0/xqKbhb5qSwH+ZHs5008yG2bEsj0SRsk2MMW3z06NGhzbIa5X5W3oz3tN1Dy8q0Sh1MmMOSRY2YyGEBr+Z/bTnwuA4UOWYpb1e7mds5Vd1VENNn9geqql5//fWquuvkFET0GDkpNYVu7AoXl6QIdSDVinmMX3coBpWwfF1WVarKlZlgVGRye7/RcUvubbntQmXJRSwfHO9h415xHjvPqoRlXpaRleA6px8+o1xjfhmcH5+h7YpmHJY0x6plsT+aUePjwTGGq/NZxgSbtpFJsyl0Y1foF7qxK9yswsfvnHxz87Cq/ut/bziNxsl49+nTp/e3jWe90I3G/3e0yNHYFfqFbuwK/UI3doV+oRu7Qr/QjV2hX+jGrtAvdGNX6Be6sSv0C93YFf4DhN43DpXD9AEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id = np.random.randint(len(faces.target))\n",
    "plt.figure(figsize = (3, 3))\n",
    "plt.imshow(faces.images[id], cmap = 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3rwC2rdyv1n"
   },
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "# input\n",
    "X = faces.images\n",
    "img_rows, img_cols = X[0].shape\n",
    "X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# output\n",
    "target = faces.target.astype(np.uint8)\n",
    "\n",
    "# Convert the target to categorical\n",
    "y = to_categorical(\n",
    "    target,\n",
    "    num_classes = len(set(target)),\n",
    "    dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hEIZT0Pyv1q"
   },
   "outputs": [],
   "source": [
    "filter = []\n",
    "# run in blocks of 10\n",
    "for i in range(len(target) // 10):\n",
    "    s = set()\n",
    "    while len(s) < 2:\n",
    "        s = set(np.random.randint(0, 10, 2, dtype = np.int8))\n",
    "    a = [x in s for x in range(10)]\n",
    "    filter.append(a)\n",
    "test = np.array(filter).flatten()\n",
    "train = np.array([not t for t in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAwmwl9fyv1s"
   },
   "outputs": [],
   "source": [
    "X_train = X[train].copy()\n",
    "X_test  = X[test].copy()\n",
    "y_train = y[train].copy()\n",
    "y_test  = y[test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P9oGZCxyv1t"
   },
   "outputs": [],
   "source": [
    "# Set up the model architecture\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMv2DgJtyv1v"
   },
   "outputs": [],
   "source": [
    "# Add a convolutional layer\n",
    "model.add(Conv2D(60,\n",
    "                 kernel_size = 4,\n",
    "                 activation = 'relu',\n",
    "                 data_format = 'channels_last',\n",
    "                 padding = 'valid',\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(30,\n",
    "                 kernel_size = 2,\n",
    "                 activation = 'relu'))\n",
    "\n",
    "# Flatten the output of the convolutional layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add an output layer for the 3 categories\n",
    "model.add(Dense(len(set(target)),\n",
    "                activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAK8acr9yv1x",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 60)        1020      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 30)        7230      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 108000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                4320040   \n",
      "=================================================================\n",
      "Total params: 4,328,290\n",
      "Trainable params: 4,328,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DkcUhjmyv11"
   },
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OCxnEJOyv12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples, validate on 64 samples\n",
      "Epoch 1/9\n",
      "256/256 [==============================] - 18s 71ms/step - loss: 4.3714 - accuracy: 0.0312 - val_loss: 5.4334 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/9\n",
      "256/256 [==============================] - 18s 69ms/step - loss: 3.3336 - accuracy: 0.1523 - val_loss: 8.6879 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/9\n",
      "256/256 [==============================] - 17s 66ms/step - loss: 2.2270 - accuracy: 0.5117 - val_loss: 16.8626 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/9\n",
      " 50/256 [====>.........................] - ETA: 15s - loss: 1.1116 - accuracy: 0.7600"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the model on a training set\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 9,\n",
    "    batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENovdR5_yv15"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuw-Lnpuyv17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size = 10)\n",
    "print('\\nTest loss: %.6f, Test accuracy: %.6f' % tuple(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZkGlsB2yv19"
   },
   "outputs": [],
   "source": [
    "def print_cm(cm):\n",
    "    d_size = max(len('%d' % cm.max()), len('%d' % cm.shape[1]))\n",
    "    if min(cm.shape) > 10: # make sparse\n",
    "        print('Sparse Matrix (*=diagonal)')\n",
    "        fmt_r = 'r%%0%dd' % d_size\n",
    "        fmt_c = ', c%%0%dd%%s= %%%dd' % (d_size, d_size)\n",
    "        for i in range(cm.shape[0]):\n",
    "            s = fmt_r % i\n",
    "            for j in range(cm.shape[1]):\n",
    "                if cm[i, j] > 0:\n",
    "                    s += fmt_c % (j, '*' if i == j else ' ', cm[i, j])\n",
    "            print(s)\n",
    "    else: # make dense\n",
    "        c = '%%%dd ' % d_size\n",
    "        s = '%s| ' % (' ' * d_size)\n",
    "        s += ''.join([c % i for i in range(len(cm[0]))])\n",
    "        print(s)\n",
    "        print('-' * len(s))\n",
    "        for i, r in enumerate(cm):\n",
    "            s = '%2d| ' % i\n",
    "            s += c * len(r)\n",
    "            print(s % tuple(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jz1anYwyv1-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test_target = np.array([x.argmax() for x in y_test])\n",
    "cm = confusion_matrix(y_test_target, predictions)\n",
    "print_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ngmNXvOyv2C"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (18, 6))\n",
    "fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(model.history.history['acc'])\n",
    "ax[0].plot(model.history.history['val_acc'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(['Train', 'Validation'])\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(model.history.history['loss'])\n",
    "ax[1].plot(model.history.history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9V0zAzJyv2E"
   },
   "outputs": [],
   "source": [
    "def implot2(im1, im2, id):\n",
    "    t1 = y_test[id].argmax()\n",
    "    t2 = predictions[id]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (6, 3))\n",
    "    fig.subplots_adjust(left = 0.02, right = 0.98, top = 0.85, wspace = 0.2)\n",
    "    fig.suptitle('Prediction %d' % id, fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "    # original image\n",
    "    ax[0].imshow(im1, cmap = 'gray')\n",
    "    ax[0].set_title('Original id: %d' % t1)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "\n",
    "    # convoluted image\n",
    "    ax[1].imshow(im2, cmap = 'gray')\n",
    "    ax[1].set_title('Test id: %d' % t2)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9x2SbfxNyv2G"
   },
   "outputs": [],
   "source": [
    "def pick_test_image(pid):\n",
    "    # pick the prediction\n",
    "    ppid = predictions[pid]\n",
    "    # find the corresponding image\n",
    "    j = -1\n",
    "    for ipid in range(test.shape[0]):\n",
    "        if test[ipid]:\n",
    "            j += 1\n",
    "        if j == pid:\n",
    "            break\n",
    "    return X[ipid].reshape(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWkU7UEGyv2J"
   },
   "outputs": [],
   "source": [
    "def pick_train_image(ppid):\n",
    "    oid = np.array([a.argmax() == ppid for a in y_train]).argmax()\n",
    "    poid = y_train[oid].argmax()\n",
    "    j = -1\n",
    "    for ioid in range(train.shape[0]):\n",
    "        if train[ioid]:\n",
    "            j += 1\n",
    "        if j == oid:\n",
    "            break\n",
    "    return X[ioid].reshape(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEnS6712yv2K"
   },
   "outputs": [],
   "source": [
    "def compare_images(id):\n",
    "    XTest = pick_test_image(id)\n",
    "    XTrain = pick_train_image(predictions[id])\n",
    "    implot2(XTrain, XTest, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEUyv-UAyv2M"
   },
   "outputs": [],
   "source": [
    "# compare one\n",
    "id = np.random.randint(len(predictions))\n",
    "compare_images(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o2EPD7syv2N",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare ten\n",
    "ids = np.random.randint(len(predictions), size = 10)\n",
    "for id in ids:\n",
    "    compare_images(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS5Tc4z9FoYy"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxI2We9OFpfs"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81DoNxN1FqGN"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2019 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gofP3k2Heop1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Demo-10-Keras_CNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
